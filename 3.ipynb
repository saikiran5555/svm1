{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7dd82de",
   "metadata": {},
   "source": [
    "The kernel trick is a technique used in Support Vector Machines (SVMs) to handle non-linearly separable data by implicitly mapping the input features into a higher-dimensional space without explicitly computing the transformation. This is achieved through the use of a kernel function.\n",
    "\n",
    "In a traditional linear SVM, the decision boundary is a hyperplane that separates the data into different classes. However, when the data is not linearly separable in the original feature space, the kernel trick allows SVMs to operate in a higher-dimensional space where the data may become linearly separable.\n",
    "\n",
    "The kernel function, denoted as \n",
    "�\n",
    "(\n",
    "�\n",
    "�\n",
    ",\n",
    "�\n",
    "�\n",
    ")\n",
    "K(x \n",
    "i\n",
    "​\n",
    " ,x \n",
    "j\n",
    "​\n",
    " ), calculates the dot product of the transformed feature vectors in the higher-dimensional space without explicitly computing the transformation. The most commonly used kernels include:\n",
    "\n",
    "Linear Kernel (\n",
    "�\n",
    "linear\n",
    "(\n",
    "�\n",
    "�\n",
    ",\n",
    "�\n",
    "�\n",
    ")\n",
    "=\n",
    "�\n",
    "�\n",
    "⋅\n",
    "�\n",
    "�\n",
    "K \n",
    "linear\n",
    "​\n",
    " (x \n",
    "i\n",
    "​\n",
    " ,x \n",
    "j\n",
    "​\n",
    " )=x \n",
    "i\n",
    "​\n",
    " ⋅x \n",
    "j\n",
    "​\n",
    " ): This corresponds to the standard dot product and is used for linearly separable data.\n",
    "\n",
    "Polynomial Kernel (\n",
    "�\n",
    "poly\n",
    "(\n",
    "�\n",
    "�\n",
    ",\n",
    "�\n",
    "�\n",
    ")\n",
    "=\n",
    "(\n",
    "�\n",
    "�\n",
    "⋅\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    ")\n",
    "�\n",
    "K \n",
    "poly\n",
    "​\n",
    " (x \n",
    "i\n",
    "​\n",
    " ,x \n",
    "j\n",
    "​\n",
    " )=(x \n",
    "i\n",
    "​\n",
    " ⋅x \n",
    "j\n",
    "​\n",
    " +c) \n",
    "d\n",
    " ): Introduces polynomial terms of degree \n",
    "�\n",
    "d and a constant \n",
    "�\n",
    "c, allowing the SVM to capture non-linear relationships.\n",
    "\n",
    "Radial Basis Function (RBF) or Gaussian Kernel (\n",
    "�\n",
    "rbf\n",
    "(\n",
    "�\n",
    "�\n",
    ",\n",
    "�\n",
    "�\n",
    ")\n",
    "=\n",
    "exp\n",
    "⁡\n",
    "(\n",
    "−\n",
    "∣\n",
    "∣\n",
    "�\n",
    "�\n",
    "−\n",
    "�\n",
    "�\n",
    "∣\n",
    "∣\n",
    "2\n",
    "2\n",
    "�\n",
    "2\n",
    ")\n",
    "K \n",
    "rbf\n",
    "​\n",
    " (x \n",
    "i\n",
    "​\n",
    " ,x \n",
    "j\n",
    "​\n",
    " )=exp(− \n",
    "2σ \n",
    "2\n",
    " \n",
    "∣∣x \n",
    "i\n",
    "​\n",
    " −x \n",
    "j\n",
    "​\n",
    " ∣∣ \n",
    "2\n",
    " \n",
    "​\n",
    " )): Maps data to an infinite-dimensional space and is particularly effective for capturing complex, non-linear patterns.\n",
    "\n",
    "The kernel trick enables SVMs to efficiently compute decision boundaries in these higher-dimensional spaces without explicitly transforming the input features. This can improve the model's ability to handle complex relationships in the data. However, the choice of the kernel and its parameters is crucial and depends on the specific characteristics of the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
